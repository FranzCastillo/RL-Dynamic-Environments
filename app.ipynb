{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Laboratorio 6\n",
    "Francisco Castillo - 21562\n",
    "\n",
    "Diego Lemus - 21"
   ],
   "id": "5e7db923447cf7ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1\n",
    "### ¿Qué es Prioritized Sweeping para ambientes determinísticos?\n",
    "Es una técnica de planeamiento que permite acelerar la propagación de los valores de los estados sin necesidad de actualizarlos todos uniformemente. En lugar de recorrer cada estado del espacio, se utiliza una cola de prioridades en la que se ordenan los estados según la magnitud del cambio que producen en sus valores. Cuando un estado se actualiza significativamente, los estados predecesores que llevan a él se incorporan a la cola para ser actualizados después. De este modo, los cambios en el valor de un estado se propagan hacia atrás de manera más eficiente, evitando cálculos innecesarios y enfocándose en las partes del espacio de estados donde realmente importa. En ambientes determinísticos esto resulta especialmente potente, ya que cada acción tiene un único resultado y la propagación de los valores puede seguir trayectorias claras.\n",
    "\n",
    "### ¿Qué es Trajectory Sampling?\n",
    "Es una técnica que evita la necesidad de explorar todo el árbol de estados y acciones posibles durante el planeamiento. En lugar de un análisis exhaustivo, el método simula trayectorias completas o episodios ficticios siguiendo la política actual y va actualizando los valores de los estados y acciones visitados en ese recorrido.\n",
    "\n",
    "### ¿Qué es Upper Confidence Bounds para Árboles (UCT)?\n",
    "Es una estrategia de selección que resuelve el dilema entre exploración y explotación. Para cada acción, combina el valor promedio estimado de los resultados obtenidos hasta el momento con un término de confianza que favorece aquellas opciones que han sido probadas pocas veces. De esta manera, el algoritmo asegura que no se quede únicamente con las ramas que parecen mejores al inicio, sino que también explore alternativas menos visitadas que podrían resultar óptimas."
   ],
   "id": "45a9e64a2bda221b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 2. MCTS",
   "id": "3418e32978b256c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, parent=None, parent_action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.children = {}\n",
    "        self.visits = 0\n",
    "        self.total_value = 0.0\n",
    "\n",
    "    def add_child(self, action, child_node):\n",
    "        self.children[action] = child_node\n",
    "\n",
    "    def is_fully_expanded(self, action_space_size):\n",
    "        return len(self.children) == action_space_size\n",
    "\n",
    "    def uct_value(self, c_param):\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        return (self.total_value / self.visits) + c_param * math.sqrt(2 * math.log(self.parent.visits) / self.visits)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, env, c_param=1.0):\n",
    "        self.env = env\n",
    "        self.c_param = c_param\n",
    "        self.root = None\n",
    "\n",
    "    def select(self):\n",
    "        current_node = self.root\n",
    "        while current_node.is_fully_expanded(self.env.action_space.n):\n",
    "            best_child = None\n",
    "            best_uct_value = -float('inf')\n",
    "            for action, child in current_node.children.items():\n",
    "                uct_val = child.uct_value(self.c_param)\n",
    "                if uct_val > best_uct_value:\n",
    "                    best_uct_value = uct_val\n",
    "                    best_child = child\n",
    "            if best_child is None:\n",
    "                 break\n",
    "            current_node = best_child\n",
    "            if self.env.unwrapped.s in self.env.unwrapped.desc.flatten().tolist() and self.env.unwrapped.desc.flatten()[self.env.unwrapped.s] in [b'H', b'G']:\n",
    "                break\n",
    "\n",
    "        return current_node\n",
    "\n",
    "    def expand(self, node):\n",
    "        if node.state is None:\n",
    "             return None\n",
    "\n",
    "        possible_actions = list(range(self.env.action_space.n))\n",
    "        random.shuffle(possible_actions)\n",
    "\n",
    "        for action in possible_actions:\n",
    "            if action not in node.children:\n",
    "                original_state = self.env.unwrapped.s\n",
    "                self.env.unwrapped.s = node.state\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "                self.env.unwrapped.s = original_state\n",
    "\n",
    "                new_node = Node(next_state, parent=node, parent_action=action)\n",
    "                node.add_child(action, new_node)\n",
    "                return new_node\n",
    "\n",
    "        return None\n",
    "\n",
    "    def simulate(self, node):\n",
    "        original_state = self.env.unwrapped.s\n",
    "        self.env.unwrapped.s = node.state\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action = self.env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if not terminated and not truncated:\n",
    "                self.env.unwrapped.s = next_state\n",
    "\n",
    "\n",
    "        self.env.unwrapped.s = original_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def backpropagate(self, node, reward):\n",
    "        current_node = node\n",
    "        while current_node is not None:\n",
    "            current_node.visits += 1\n",
    "            current_node.total_value += reward\n",
    "            current_node = current_node.parent\n",
    "\n",
    "    def run_mcts(self, num_simulations):\n",
    "        if self.root is None:\n",
    "            return\n",
    "\n",
    "        for _ in range(num_simulations):\n",
    "            leaf_node = self.select()\n",
    "\n",
    "            if self.env.unwrapped.s in self.env.unwrapped.desc.flatten().tolist() and self.env.unwrapped.desc.flatten()[self.env.unwrapped.s] in [b'H', b'G']:\n",
    "                original_state = self.env.unwrapped.s\n",
    "                self.env.unwrapped.s = leaf_node.state\n",
    "                reward = 0\n",
    "                if self.env.unwrapped.desc.flatten()[self.env.unwrapped.s] == b'G':\n",
    "                     reward = 1.0\n",
    "                self.env.unwrapped.s = original_state\n",
    "                self.backpropagate(leaf_node, reward)\n",
    "                continue\n",
    "\n",
    "            new_node = self.expand(leaf_node)\n",
    "\n",
    "            if new_node is None:\n",
    "                simulation_reward = self.simulate(leaf_node)\n",
    "                self.backpropagate(leaf_node, simulation_reward)\n",
    "            else:\n",
    "                simulation_reward = self.simulate(new_node)\n",
    "                self.backpropagate(new_node, simulation_reward)\n",
    "\n",
    "    def get_best_action(self):\n",
    "        if self.root is None or not self.root.children:\n",
    "            return None\n",
    "\n",
    "        best_action = None\n",
    "        best_visits = -1\n",
    "\n",
    "        for action, child in self.root.children.items():\n",
    "            if child.visits > best_visits:\n",
    "                best_visits = child.visits\n",
    "                best_action = action\n",
    "\n",
    "        return best_action\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "num_episodes = 100\n",
    "num_simulations_per_step = 5000\n",
    "\n",
    "episode_rewards = []\n",
    "successes = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    mcts = MCTS(env, c_param=1.0)\n",
    "    total_episode_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        mcts.root = Node(observation)\n",
    "        mcts.run_mcts(num_simulations_per_step)\n",
    "        action = mcts.get_best_action()\n",
    "\n",
    "        if action is None:\n",
    "            break\n",
    "\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_episode_reward += reward\n",
    "        observation = next_observation\n",
    "        step_count += 1\n",
    "\n",
    "    if reward > 0:\n",
    "        successes += 1\n",
    "\n",
    "    episode_rewards.append(total_episode_reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "success_rate = (successes / num_episodes) * 100\n",
    "average_reward = np.mean(episode_rewards)\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "print(f\"Average Reward per Episode: {average_reward:.4f}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12f494a5b6b7a78f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
