{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Laboratorio 6\n",
    "Francisco Castillo - 21562\n",
    "\n",
    "Diego Lemus - 21"
   ],
   "id": "5e7db923447cf7ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1\n",
    "### ¿Qué es Prioritized Sweeping para ambientes determinísticos?\n",
    "Es una técnica de planeamiento que permite acelerar la propagación de los valores de los estados sin necesidad de actualizarlos todos uniformemente. En lugar de recorrer cada estado del espacio, se utiliza una cola de prioridades en la que se ordenan los estados según la magnitud del cambio que producen en sus valores. Cuando un estado se actualiza significativamente, los estados predecesores que llevan a él se incorporan a la cola para ser actualizados después. De este modo, los cambios en el valor de un estado se propagan hacia atrás de manera más eficiente, evitando cálculos innecesarios y enfocándose en las partes del espacio de estados donde realmente importa. En ambientes determinísticos esto resulta especialmente potente, ya que cada acción tiene un único resultado y la propagación de los valores puede seguir trayectorias claras.\n",
    "\n",
    "### ¿Qué es Trajectory Sampling?\n",
    "Es una técnica que evita la necesidad de explorar todo el árbol de estados y acciones posibles durante el planeamiento. En lugar de un análisis exhaustivo, el método simula trayectorias completas o episodios ficticios siguiendo la política actual y va actualizando los valores de los estados y acciones visitados en ese recorrido.\n",
    "\n",
    "### ¿Qué es Upper Confidence Bounds para Árboles (UCT)?\n",
    "Es una estrategia de selección que resuelve el dilema entre exploración y explotación. Para cada acción, combina el valor promedio estimado de los resultados obtenidos hasta el momento con un término de confianza que favorece aquellas opciones que han sido probadas pocas veces. De esta manera, el algoritmo asegura que no se quede únicamente con las ramas que parecen mejores al inicio, sino que también explore alternativas menos visitadas que podrían resultar óptimas."
   ],
   "id": "45a9e64a2bda221b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 2. MCTS",
   "id": "3418e32978b256c8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-22T01:06:09.751695Z",
     "start_time": "2025-08-22T01:06:06.092048Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = {}  # Dictionary mapping actions to child nodes\n",
    "        self.visits = 0\n",
    "        self.total_reward = 0\n",
    "        self.action = action # The action taken from the parent to reach this node\n",
    "\n",
    "    def is_terminal(self, env):\n",
    "        \"\"\"Checks if the current state is a terminal state using information from env.unwrapped.desc.\"\"\"\n",
    "        unwrapped_env = env.unwrapped\n",
    "        row, col = np.unravel_index(self.state, unwrapped_env.desc.shape)\n",
    "        cell_type = unwrapped_env.desc[row, col].decode('utf-8')\n",
    "        return cell_type in ['H', 'G']\n",
    "\n",
    "\n",
    "    def is_fully_expanded(self, env):\n",
    "        \"\"\"Checks if all possible actions from this state have a corresponding child node.\"\"\"\n",
    "        num_actions = env.action_space.n\n",
    "        return len(self.children) == num_actions\n",
    "\n",
    "    def get_untried_actions(self, env):\n",
    "        \"\"\"Returns a list of actions that have not been tried from this node.\"\"\"\n",
    "        all_actions = list(range(env.action_space.n))\n",
    "        tried_actions = self.children.keys()\n",
    "        return [action for action in all_actions if action not in tried_actions]\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"Manages the MCTS process.\"\"\"\n",
    "    def __init__(self, env, c_param=1.0):\n",
    "        self.env = env\n",
    "        initial_observation, info = self.env.reset()\n",
    "        self.root = Node(initial_observation)  # Root node representing the initial state\n",
    "        self.c_param = c_param\n",
    "\n",
    "    def select(self, node):\n",
    "        \"\"\"Selects the best child node based on UCT.\"\"\"\n",
    "        while not node.is_terminal(self.env) and node.is_fully_expanded(self.env):\n",
    "             node = self._best_child(node, self.c_param)\n",
    "        return node\n",
    "\n",
    "    def expand(self, node):\n",
    "        \"\"\"Adds a new child node for an untried action\"\"\"\n",
    "        if not node.is_terminal(self.env) and not node.is_fully_expanded(self.env):\n",
    "            untried_actions = node.get_untried_actions(self.env)\n",
    "            if untried_actions:\n",
    "                action = np.random.choice(untried_actions)\n",
    "                unwrapped_env = self.env.unwrapped\n",
    "                possible_outcomes = unwrapped_env.P[node.state][action]\n",
    "\n",
    "                if possible_outcomes:\n",
    "                    # Pick the first outcome for expansion.\n",
    "                    probability, next_state, reward, done = possible_outcomes[0]\n",
    "                    new_node = Node(next_state, parent=node, action=action)\n",
    "                    node.children[action] = new_node\n",
    "                    return new_node\n",
    "        return node\n",
    "\n",
    "\n",
    "    def simulate(self, node):\n",
    "        \"\"\"Performs a simulation from the given node's state using env.unwrapped.P.\"\"\"\n",
    "        current_state = node.state\n",
    "        total_rollout_reward = 0\n",
    "        done = node.is_terminal(self.env)\n",
    "\n",
    "        temp_state = current_state\n",
    "        unwrapped_env = self.env.unwrapped\n",
    "\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()\n",
    "\n",
    "            possible_outcomes = unwrapped_env.P[temp_state][action]\n",
    "\n",
    "            if not possible_outcomes:\n",
    "                break\n",
    "\n",
    "            outcomes_probs = [outcome[0] for outcome in possible_outcomes]\n",
    "            if not np.isclose(sum(outcomes_probs), 1.0):\n",
    "                 outcomes_probs = outcomes_probs / np.sum(outcomes_probs)\n",
    "\n",
    "            chosen_outcome_index = np.random.choice(len(possible_outcomes), p=outcomes_probs)\n",
    "            probability, next_state, reward, done_outcome = possible_outcomes[chosen_outcome_index]\n",
    "\n",
    "            temp_state = next_state\n",
    "            total_rollout_reward += reward\n",
    "\n",
    "            if Node(temp_state).is_terminal(self.env):\n",
    "                 done = True\n",
    "\n",
    "\n",
    "        return total_rollout_reward\n",
    "\n",
    "\n",
    "    def backpropagate(self, node, reward):\n",
    "        \"\"\"Backpropagates the reward up the tree.\"\"\"\n",
    "        current_node = node\n",
    "        while current_node is not None:\n",
    "            current_node.visits += 1\n",
    "            current_node.total_reward += reward\n",
    "            current_node = current_node.parent\n",
    "\n",
    "    def _uct(self, node, c_param):\n",
    "        \"\"\"Calculates the UCT value for a child node.\"\"\"\n",
    "        if node.visits == 0:\n",
    "            return float('inf')\n",
    "        if node.parent is None or node.parent.visits == 0:\n",
    "             return float('inf')\n",
    "\n",
    "        return (node.total_reward / node.visits) + c_param * np.sqrt(np.log(node.parent.visits) / node.visits)\n",
    "\n",
    "    def _best_child(self, node, c_param):\n",
    "        \"\"\"Selects the child with the highest UCT value.\"\"\"\n",
    "        best_child = None\n",
    "        best_uct_value = -float('inf')\n",
    "        for action, child in node.children.items():\n",
    "            uct_value = self._uct(child, c_param)\n",
    "            if uct_value > best_uct_value:\n",
    "                best_uct_value = uct_value\n",
    "                best_action = action\n",
    "                best_child = child\n",
    "\n",
    "        return best_child\n",
    "\n",
    "\n",
    "    def run_mcts(self, num_simulations):\n",
    "        \"\"\"Runs the MCTS algorithm for a given number of simulations.\"\"\"\n",
    "        for _ in range(num_simulations):\n",
    "            leaf_node = self.select(self.root)\n",
    "            node_to_simulate_from = self.expand(leaf_node)\n",
    "            rollout_reward = self.simulate(node_to_simulate_from)\n",
    "            self.backpropagate(node_to_simulate_from, rollout_reward)\n",
    "\n",
    "    def get_best_action(self):\n",
    "        \"\"\"Returns the action from the root node that leads to the child with the most visits.\"\"\"\n",
    "        if not self.root.children:\n",
    "            return None\n",
    "\n",
    "        best_action = None\n",
    "        max_visits = -1\n",
    "        for action, child_node in self.root.children.items():\n",
    "            if child_node.visits > max_visits:\n",
    "                max_visits = child_node.visits\n",
    "                best_action = action\n",
    "        return best_action"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5000 MCTS simulations...\n",
      "MCTS simulations finished.\n",
      "Recommended action from MCTS: 2\n",
      "Taking action 2 in the environment.\n",
      "Action taken: 2\n",
      "Resulting state: 0\n",
      "Reward: 0.0\n",
      "Terminated: False\n",
      "Truncated: False\n",
      "Environment closed.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-22T01:06:10.501765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment and MCTS parameters\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "num_episodes = 100\n",
    "num_simulations_per_step = 5000\n",
    "\n",
    "episode_rewards = []\n",
    "successes = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    mcts = MCTS(env, c_param=1.0)\n",
    "    total_episode_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        mcts.root = Node(observation)\n",
    "        mcts.run_mcts(num_simulations_per_step)\n",
    "        action = mcts.get_best_action()\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_episode_reward += reward\n",
    "        observation = next_observation\n",
    "        step_count += 1\n",
    "    if reward > 0:\n",
    "        successes += 1\n",
    "\n",
    "    episode_rewards.append(total_episode_reward)\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print(f\"Completed episode {episode + 1}/{num_episodes}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "success_rate = (successes / num_episodes) * 100\n",
    "average_reward = np.mean(episode_rewards)\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Success Rate: {success_rate:.2f}%\")\n",
    "print(f\"Average Reward per Episode: {average_reward:.4f}\")"
   ],
   "id": "12f494a5b6b7a78f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MCTS agent over 100 episodes with 5000 simulations per step.\n",
      "Completed episode 10/100\n",
      "Completed episode 20/100\n",
      "Completed episode 30/100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d4bfa13b93bab6dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
